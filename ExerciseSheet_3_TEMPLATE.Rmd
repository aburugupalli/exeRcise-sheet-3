---
title: "Exercise #3"
subtitle: "Fortgeschrittene Statistische Software für NF"
author: "Anay Burugupalli (12750156), Metin Istutan (12349723), ..."
date: "`r Sys.Date()`"
output: distill::distill_article
---

## General Remarks

-   You can submit your solutions in teams of up to 3 students.
-   Include all your team-member's names and student numbers
    (Matrikelnummern) in the `authors` field.
-   Please use the exercise template document to work on and submit your
    results.
-   Use a level 2 heading for each new exercise and answer each subtask
    next to its bullet points or use a new level 3 heading if you want.
-   Always render the R code for your solutions (`echo=TRUE`) and make
    sure to include the resulting data in your rendered document.
    -   Make sure to not print more than 10 rows of data (unless
        specifically instructed to).
-   Always submit both the rendered document(s) as well as your source
    Rmarkdown or Quarto document. Submit the files separately on moodle,
    **not** as a zip archive.
-   Submission format is HTML. Other formats will lead to a deduction of
    points.
    
```{r}

# Setup-Chunk
knitr::opts_chunk$set(echo = TRUE)
library(ggplot2)
library(dplyr)
library(knitr)
library(broom)
library(easystats)

```

## Exercise 1: Initializing git (4 Points)

For this whole exercise sheet we will be tracking all our changes to it
in git.

a)  Start by initializing a new R project with git support, called
    `2025-exeRcise-sheet-3`. If you forgot how to do this, you can
    follow this
    [guide](https://malikaihle.github.io/Introduction-RStudio-Git-GitHub/rstudio_project.html).
b)  Commit the files generated by Rstudio.
c)  For all of the following tasks in this exercise sheet we ask you to
    always commit your changes after finishing each subtask e.g. create
    a commit after task *1d*, *1e* etc.

> Note: This applies only to answers that have text or code as their
> answer. If you complete tasks in a different order or forget to commit
> one, this is no problem. If you change your answers you can just
> create multiple commits to track the changes.

d)  Name 2 strengths and 2 weaknesses of git. (Don't forget to create a
    commit after this answer, see *1c*)
    
    Strengths:
    - Hard to lose files with git
    - good for collaboration
    
    Weaknesses:
    - complicated to use
    - struggles with large files
    
e)  Knit this exercise sheet. Some new files will automatically be
    generated when knitting the sheet e.g. the HTML page. Ignore these
    files, as we only want to track the source files themselves. You
    can, but don't need to create a `.gitignore` file. Just do not
    commit these files manually.

## Exercise 2: Putting your Repository on GitHub (3 Points)

For this task you will upload your solution to GitHub.

a)  Create a new repository on GitHub in your account named
    `exeRcise-sheet-3`. Make sure you create a **public repository** so
    we are able to see it for grading. Add the link to the repository
    below:
    
    https://github.com/aburugupalli/exeRcise-sheet-3
    
b)  Push your code to this new repository by copying and executing the
    snippet on github listed under
    `…or push an existing repository from the command line`.
c)  Regularly push your latest changes to GitHub again and especially do
    so when you are finished with this sheet.

## Exercise 3: Pixar Films (4 Points)

Download the `pixar_films` and `public_response` datasets from the
GitHub repository and track them in git.

Link:
<https://github.com/rfordatascience/tidytuesday/tree/main/data/2025/2025-03-11>

For small datasets like these adding them to git is not a problem.

a)  Load the `pixar_films` dataset into R. Clean the dataset by removing
    films without a title. Inspect the variable `film_rating`. What are
    the possible values and what do they mean? Create a factor variable
    for the film rating. Why is this appropriate?
    
```{r pixar}

# a) Datensatz laden und bereinigen
library(tidytuesdayR)

# Datensatz laden
tuesdata <- tidytuesdayR::tt_load(2025, week = 10)
pixar_films <- tuesdata$pixar_films

print(colnames(pixar_films))

# Bereinigen - Filme ohne Titel entfernen
pixar_clean <- pixar_films %>%
  filter(!is.na(film), film != "", film != "NA")

# Inspektion der film_rating Variable
unique(pixar_clean$film_rating)
table(pixar_clean$film_rating)

# Faktor-Variable erstellen
pixar_clean$film_rating_factor <- factor(
  pixar_clean$film_rating,
  levels = c("G", "PG", "N/A"),
  ordered = TRUE
)

summary(pixar_clean$film_rating_factor)


```
    
    G: Suitable for all age groups - no material that parents would consider objectionable for children
    PG: Parental guidance recommended - some content may be unsuitable for children
    N/A: No rating available
    
    Why Factor Variable? Even though film ratings are categories, not numbers they still have a hierarchy among them selves.

<!-- -->

b)  Inspect the film titles manually. Which films form a film series? A
    film series can be identified by a common word in the titles of the
    films, often in conjunction with a number in the title,
    e.g. "Despicable Me" and "Despicable Me 2". Create a dataframe which
    displays a list of the different series with the titles of the films
    and how many films belong to the series. Output the dataframe.
    
```{r}

# Identify film series
pixar_clean$series <- case_when(
  grepl("Toy Story", pixar_clean$film) ~ "Toy Story",
  grepl("Cars", pixar_clean$film) ~ "Cars", 
  grepl("Monsters", pixar_clean$film) ~ "Monsters",
  grepl("Finding", pixar_clean$film) ~ "Finding",
  grepl("Incredibles", pixar_clean$film) ~ "Incredibles",
  TRUE ~ "Standalone"
)

# Create series dataframe
series_films <- pixar_clean %>%
  filter(series != "Standalone") %>%
  group_by(series) %>%
  mutate(films_in_series = n()) %>%
  select(series, film, films_in_series) %>%
  arrange(series, film)

kable(series_films)


```
    

c)  Load the `public_response` dataframe into R. Convert the
    `cinema_score` variable into a factor while ensuring the factor
    levels are defined in ascending order, from the lowest to the
    highest score. Combine `public_response` with the `pixar_films`
    dataset using an appropriate merge variable.
    
```{r}

# 1. Load the public_response dataset
public_response <- tuesdata$public_response

# 2. Examine the structure of public_response
cat("=== PUBLIC_RESPONSE DATASET STRUCTURE ===\n")
str(public_response)
head(public_response)

# 3. Examine cinema_score variable
cat("\n=== CINEMA_SCORE ANALYSIS ===\n")
cat("Unique values in cinema_score:\n")
print(unique(public_response$cinema_score))
cat("\nFrequency table:\n")
print(table(public_response$cinema_score))

# 4. Convert cinema_score to ordered factor (ascending order: lowest to highest)
public_response$cinema_score_factor <- factor(
  public_response$cinema_score,
  levels = c("F", "D", "D+", "C-", "C", "C+", "B-", "B", "B+", "A-", "A", "A+"),
  ordered = TRUE
)

# 5. Verify the factor conversion
cat("\n=== FACTOR CONVERSION VERIFICATION ===\n")
cat("Factor summary:\n")
print(summary(public_response$cinema_score_factor))
cat("Is ordered:", is.ordered(public_response$cinema_score_factor), "\n")
cat("Factor levels:", paste(levels(public_response$cinema_score_factor), collapse = " < "), "\n")

# 6. Identify merge variable by checking column names
cat("\n=== IDENTIFYING MERGE VARIABLE ===\n")
cat("Columns in pixar_clean:\n")
print(colnames(pixar_clean))
cat("Columns in public_response:\n")
print(colnames(public_response))

# 7. Merge the datasets (using appropriate common variable)
# Most likely merge variable is 'film' (film title)
combined_data <- merge(
  pixar_clean, 
  public_response, 
  by = "film",           # Common variable (adjust if different)
  all.x = TRUE          # Left join: keep all Pixar films
)

# 8. Examine the merged dataset
cat("\n=== MERGED DATASET ANALYSIS ===\n")
cat("Original pixar_clean rows:", nrow(pixar_clean), "\n")
cat("Public_response rows:", nrow(public_response), "\n")
cat("Combined_data rows:", nrow(combined_data), "\n")
cat("Successful matches:", sum(!is.na(combined_data$cinema_score_factor)), "\n")

# 9. Display structure and sample of merged data
str(combined_data)
head(combined_data)

# 10. Check for films without public response data
films_without_scores <- combined_data[is.na(combined_data$cinema_score), "film"]
if(length(films_without_scores) > 0) {
  cat("\nFilms without cinema scores:\n")
  print(films_without_scores)
}

```
    

d)  Choose one of the variables representing the public response and
    create a bar plot for the films belonging to a series. Here are the
    details of the plot:

    -   The film series are represented on the x-axis.
    -   Your chosen public response variable is displayed on the y-axis.
    -   Each film in the series is represented as a separate bar. Bars
        are grouped by film under their respective series on the x-axis.
        Order the bars within a series according to the release date of
        the films.
    -   A title and axis labels for context.

    What do you notice when comparing the scores of the films in a
    series? Do you see any patterns?
    
```{r}

library(ggplot2)
library(dplyr)

# Filter for series films and order by release date
series_data <- combined_data %>%
  filter(series != "Standalone") %>%
  arrange(series, release_date)

# Convert cinema_score to numeric for plotting
score_levels <- c("F", "D", "D+", "C-", "C", "C+", "B-", "B", "B+", "A-", "A", "A+")
series_data$score_numeric <- as.numeric(factor(series_data$cinema_score_factor, levels = score_levels))

# Create the grouped bar plot
ggplot(series_data, aes(x = series, y = score_numeric, fill = film)) +
  geom_col(position = position_dodge(width = 0.8), 
           color = "black", alpha = 0.8) +
  geom_text(aes(label = cinema_score), 
            position = position_dodge(width = 0.8),
            vjust = -0.5, fontweight = "bold") +
  scale_y_continuous(breaks = 1:12, labels = score_levels, limits = c(0, 13)) +
  labs(
    title = "CinemaScore Ratings for Pixar Film Series",
    subtitle = "Films ordered by release date within each series",
    x = "Film Series",
    y = "CinemaScore Rating",
    fill = "Film"
  ) +
  theme_minimal() +
  theme(
    plot.title = element_text(size = 14, face = "bold"),
    axis.title = element_text(size = 12, face = "bold"),
    legend.position = "bottom"
  )

```
    

## Exercise 4: Open Analysis (4 points)

This exercise is a bit more open-ended. You can choose any dataset from
[Our World in Data](https://ourworldindata.org/) and analyze it, while
determining the research question yourself.

a)  Go to <https://github.com/owid/owid-datasets/tree/master/datasets>
    and choose a dataset that interests you. You can have a look at
    <https://ourworldindata.org/> to gather some inspiration.
b)  Download the dataset and track it in git.
c)  Put the name / title of the dataset and a link to it below.

-   Dataset Name: Airline hijacking
-   Link: <https://github.com/owid/owid-datasets/tree/master/datasets/Airline%20hijacking%20-%20Aviation%20Safety%20Network>...

d)  Come up with a (research) question you want to answer with the data
    and briefly explain why you believe this is an interesting question
    within one sentence. It should be a question that can be answered
    with the dataset and using R.
    
    Did the introduction of enhanced airport security measures following major hijacking incidents lead to a measurable decline in both the frequency and lethality of airline hijackings, and can we identify specific turning points in the time series data that correspond to known security policy changes?
    
e)  Use R to answer your chosen question. Please limit your analysis to
    the functions and techniques we have covered so far in the course.
    You are **not expected** to use advanced statistical models or
    external packages which haven't been introduced.
    
```{r}

# Load the dataset
data <- read.csv("data/Airline hijacking - Aviation Safety Network.csv")

# Clean column names for easier handling
colnames(data) <- c("Entity", "Year", "Hijackings", "Fatalities")

# Calculate basic statistics by decades to identify trends
data$Decade <- floor(data$Year / 10) * 10
decade_stats <- aggregate(cbind(Hijackings, Fatalities) ~ Decade, data = data, FUN = mean)
print("Average hijackings and fatalities by decade:")
print(decade_stats)

# Calculate fatality rate (fatalities per hijacking) to assess lethality changes
data$Fatality_Rate <- ifelse(data$Hijackings > 0, data$Fatalities / data$Hijackings, 0)

# Compare pre and post major security implementation periods
# Using 1973 as a key turning point (introduction of mandatory screening)
pre_1973 <- data[data$Year < 1973, ]
post_1973 <- data[data$Year >= 1973, ]

cat("Pre-1973 (before mandatory screening):\n")
cat("Average hijackings per year:", mean(pre_1973$Hijackings), "\n")
cat("Average fatalities per year:", mean(pre_1973$Fatalities), "\n")

cat("\nPost-1973 (after mandatory screening):\n")
cat("Average hijackings per year:", mean(post_1973$Hijackings), "\n")
cat("Average fatalities per year:", mean(post_1973$Fatalities), "\n")

# Test for significant difference using t-test
hijacking_test <- t.test(pre_1973$Hijackings, post_1973$Hijackings)
print("T-test results for hijackings before vs after 1973:")
print(hijacking_test)

# Analyze post-2001 period (enhanced security post-9/11)
post_2001 <- data[data$Year >= 2001, ]
cat("\nPost-2001 period (enhanced security post-9/11):\n")
cat("Average hijackings per year:", mean(post_2001$Hijackings), "\n")
cat("Average fatalities per year:", mean(post_2001$Fatalities), "\n")

```
    
    
f)  Create a meaningful plot / figure with the dataset. Make sure to
    provide a figure caption (via the chunk options / Rmarkdown) and
    correctly label the figure.
    
```{r main-plot, fig.cap="Figure 1: Airline hijackings and fatalities over time (1942-2014) showing the impact of major security policy implementations. The blue dashed line marks 1973 when mandatory passenger screening was introduced, and the green dashed line marks 2001 when enhanced security measures were implemented following 9/11. The orange dashed line shows the smoothed trend for hijackings, clearly demonstrating the decline following security interventions.", fig.width=12, fig.height=8}

plot(data$Year, data$Hijackings, 
     type = "l", 
     lwd = 2, 
     col = "red",
     xlab = "Year", 
     ylab = "Count",
     main = "Airline Hijackings and Fatalities: Impact of Security Measures (1942-2014)",
     ylim = c(0, max(c(data$Hijackings, data$Fatalities))))

# Add fatalities line
lines(data$Year, data$Fatalities, lwd = 2, col = "darkred")

# Add milestone lines for key security implementations
abline(v = 1973, col = "blue", lty = 2, lwd = 2)    # Mandatory screening
abline(v = 2001, col = "green", lty = 2, lwd = 2)   # Post-9/11 security

# Add text annotations
text(1973, max(data$Hijackings) * 0.9, "Mandatory\nScreening\n(1973)", 
     pos = 4, cex = 0.8, col = "blue")
text(2001, max(data$Hijackings) * 0.8, "Enhanced\nSecurity\n(2001)", 
     pos = 4, cex = 0.8, col = "green")

# Add smooth trend line for hijackings
hijack_smooth <- lowess(data$Year, data$Hijackings, f = 0.3)
lines(hijack_smooth$x, hijack_smooth$y, col = "orange", lwd = 3, lty = 3)

# Add legend
legend("topright", 
       legend = c("Number of Hijackings", "Number of Fatalities", 
                  "Hijacking Trend", "Security Milestones"),
       col = c("red", "darkred", "orange", "blue"),
       lty = c(1, 1, 3, 2),
       lwd = c(2, 2, 3, 2),
       cex = 0.9)

```
    

## Final Note

Make sure to push all your commits and changes to GitHub before
submitting the exercise sheet.
